{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ![alt text](https://gluon-cv.mxnet.io/_static/gluon-logo.svg \"Gluon Logo\")\n",
    "  \n",
    "  #  GluonCV: a Deep Learning Toolkit for Computer Vision\n",
    "\n",
    "GluonCV provides implementations of state-of-the-art (SOTA) deep learning algorithms in computer vision. It aims to help engineers, researchers, and students quickly prototype products, validate new ideas and learn computer vision.\n",
    "\n",
    "GluonCV features:\n",
    "\n",
    "   * training scripts that reproduce SOTA results reported in latest papers,\n",
    "\n",
    "   * a large set of pre-trained models,\n",
    "\n",
    "   * carefully designed APIs and easy to understand implementations,\n",
    "\n",
    "   * community support.\n",
    "\n",
    "   \n",
    "![alt text](images/gluoncv.png \"Gluoncv Applications\")\n",
    "\n",
    "This notebook will focus on training a <b>custom Object Detection model</b> using the <b>SSD</b> network\n",
    "(custom means we are not using a pre-trained model trained on a dataset such as ImageNet)\n",
    "\n",
    "In order to use the GluonCV library, we must install it by updating the version of mxnet that is installed.\n",
    "We will also update some paths to the CUDA libraries, a dependency of GluonCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3                            # AWS Python framework                            \n",
    "import os                               # OS library to access file paths\n",
    "from gluoncv import data, utils         # gluoncv data and utils modules to create datasets\n",
    "from gluoncv.data import VOCDetection   # VOCDetection allows gluoncv to recognize our boundingboxes and classes\n",
    "from gluoncv.utils import viz           # gluoncv specific visualization capabilities\n",
    "from matplotlib import pyplot as plt    # visualization capabilites (to view dataset samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   We are going to use a PASCAL VOC formatted dataset for this model.  We will briefly cover the formatting of a VOC Dataset in this notebook, but for more information about PASCAL VOC, visit: https://gluon-cv.mxnet.io/build/examples_datasets/pascal_voc.html#sphx-glr-build-examples-datasets-pascal-voc-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has <b>52 classes</b> - corresponding to the 52 different cards in a deck of playing cards (minus the Jokers)\n",
    "![alt text](images/playingcards.png \"Deck of cards\")\n",
    "\n",
    "The class names are abbreviated by first letter of rank and first letter of suite:\n",
    "\n",
    "    2C = Two of Clubs\n",
    "    AS = Ace of Spades\n",
    "    ...\n",
    "\n",
    "Here we will create a class derived from the VOCDetection method for our custom dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move sample data from S3 to our notebook instance\n",
    "We have a small sample dataset that we will evaluate on this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive s3://gluoncv-notebook ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us take a look a the directory structure of a typical PASCAL VOC dataset:\n",
    "!tree VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the images and annotations in \"VOCvalidate\" as our validation dataset and the images and annotations in \"VOCtrain\" as our training dataset.  The \"VOCvalidate\" folder could be named anything meaningful. In this example since we have only 1 training and 1 validation dataset we keep the names simple and refer to them as VOCtrain and VOCvalidate, but we will see momentarily why the folders \"VOCvalidate\" & \"VOCtrain\" are not synonymous with datasets, instead we will refer to them as VOC Imageset folders. \n",
    "\n",
    "**note** The word VOC must be the first three letters of the ImageSet foldernames or the VOCDetection method that will be introduced later will not recognize the folders.\n",
    "\n",
    "Within each VOC Imageset folder there are 3 child folders:\n",
    "    Annotations\n",
    "    ImageSets\n",
    "    JPEGImages\n",
    "    \n",
    "The <b>Annotations<b> folder holds .XML files.  Each XML file contains bounding box information for 1 image file in the dataset. That image file may have multiple objects, but there is a 1:1 relationship between annotation files and imamge files.  Let's look at one of the Annotation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat VOC/VOCvalidate/Annotations/aug3_046386182.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above annotation file, there are 4 object nodes.  The image (which we will see soon, contains two playing cards. Each playing card has two locations for rank and suite and in this image, all 4 locations are visible.\n",
    "\n",
    "Each object node contains the class name (QC, 8H) as well as the bounding box for the rank and suite. It is important to note the this model ONLY detects the rank and suite of playing cards, not the entire card or the number of suite icons on the card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second child folder is ImageSets. This folder contains text files that list the images you wish to include in a particular dataset. Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat VOC/VOCvalidate/ImageSets/Main/val.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with a VOC dataset for object detection, we use a child folder called \"Main\" within the ImageSets folder.  If we were using gluoncv for other task such as Action/Event, Pose Detection or Segmentation we would create additional folders at the level of Main and give them names corresponding to the type of model. Within the Main folder is a file called val.txt. It is used to encapsulate the size of the dataset we wish to use by listing each image name. In this example, we only include 5 files.  You will note the file extension is absent.  The PASCAL VOC format will expect to find an annotation file (In the Annotation directory and ending in .xml) of the same name as each entry in this val.txt.  PASCAL VOC will also expect to find an image file of the same name in the JPEGImages directory with a .jpg extension.\n",
    "\n",
    "This structure allows you to store n files in the annotations and JPEGImages directories, and then customize an ImageSet listing file to only train/validate on selected annotation and JPEGImage files.  This gives you the capability to store as many images as you wish in a single directory, but create separate datasets by creating multiple ImageSet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final directory is the JPEGImages folder which as stated above, contains the corresponding image to the annotations file. We will use the GluonCV API to explore these files further, first however we need to introduce gluoncv to our class structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCLike(VOCDetection):\n",
    "    CLASSES = [\"ac\", \"2c\", \"3c\", \"4c\", \"5c\", \"6c\", \"7c\", \"8c\", \"9c\", \"10c\", \"jc\", \"qc\", \"kc\", \"ad\", \"2d\", \"3d\", \"4d\", \"5d\", \"6d\", \"7d\", \"8d\", \"9d\", \"10d\", \"jd\", \"qd\", \"kd\", \"ah\", \"2h\", \"3h\", \"4h\", \"5h\", \"6h\", \"7h\", \"8h\", \"9h\", \"10h\", \"jh\", \"qh\", \"kh\", \"as\", \"2s\", \"3s\", \"4s\", \"5s\", \"6s\", \"7s\", \"8s\", \"9s\", \"10s\", \"js\", \"qs\", \"ks\"]\n",
    "    def __init__(self, root, splits, transform=None, index_map=None, preload_label=True):\n",
    "        super(VOCLike, self).__init__(root, splits, transform, index_map, preload_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create an object containing our classes for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classes = [\"ac\", \"2c\", \"3c\", \"4c\", \"5c\", \"6c\", \"7c\", \"8c\", \"9c\", \"10c\", \"jc\", \"qc\", \"kc\", \"ad\", \"2d\", \"3d\", \"4d\", \"5d\",\n",
    "           \"6d\", \"7d\", \"8d\", \"9d\", \"10d\", \"jd\", \"qd\", \"kd\", \"ah\", \"2h\", \"3h\", \"4h\", \"5h\", \"6h\", \"7h\", \"8h\", \"9h\", \"10h\",\n",
    "           \"jh\", \"qh\", \"kh\", \"as\", \"2s\", \"3s\", \"4s\", \"5s\", \"6s\", \"7s\", \"8s\", \"9s\", \"10s\", \"js\", \"qs\", \"ks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.utils.metrics.voc_detection import VOC07MApMetric\n",
    "\n",
    "# Use our newly created class to generate a reference to the training data\n",
    "train_dataset = VOCLike(root='VOCTemplate', splits=(('VOCTrain', 'train'),))\n",
    "    \n",
    "# Use our newly created class to generate a reference to the validation data\n",
    "val_dataset = VOCLike(root='VOCTemplate', splits=(('VOCValid', 'valid'),))\n",
    "\n",
    "# This metric will be introduced later prior to training\n",
    "val_metric = VOC07MApMetric(iou_thresh=0.5, class_names=val_dataset.classes)\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Validation images:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our label (annotation + classname) data is currently in a separate file from our image.  We can use the GluonCV library to read an image-label pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a training image and corresponding label\n",
    "train_image, train_label = train_dataset[24000]\n",
    "\n",
    "\n",
    "# The train_image is an mxnet.ndarray that should be a 720 x 720 RGB image\n",
    "print(\"train_image shape:{}\".format(train_image.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will take a moment to pay special attention to the shape of the train_label\n",
    "\n",
    "# the label is a numpy array \n",
    "print(\"train_label type: {}\".format(type(train_label)))\n",
    "\n",
    "# the array has n elements - 1 element for each object in the train image\n",
    "print(\"train_label shape: {}\".format(train_label.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the entire array:\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 4 positions in an element are the bounding box (xmin, ymin, xmax, ymax)\n",
    "The 5th position is the class ID\n",
    "The 6th position is the label, if it has been pre-loaded\n",
    "\n",
    "![alt text](images/label_shape.png \"Label shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view this image with bounding boxes and classes\n",
    "bboxes = train_label[:, :4]   # Get all elements :, and get all positions up to the 4th :4\n",
    "cids = train_label[:, 4:5]    # Get the class ID in the 4th position\n",
    "print('image:', train_image.shape)\n",
    "print('bboxes:', bboxes.shape, 'class ids:', cids.shape)\n",
    "ax = viz.plot_bbox(train_image.asnumpy(), bboxes, labels=cids, class_names=train_dataset.classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.mxnet import MXNetModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "model_artifacts_location = 's3://{}'.format(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "\n",
    "git_config = {'repo': 'https://github.com/Christopheraburns/cv-at-edge.git',\n",
    "              'branch': 'master'}\n",
    "\n",
    "\n",
    "estimator = MXNet(entry_point=\"train_ssd-playing-cards.py\",\n",
    "          role=role,\n",
    "          git_config=git_config,\n",
    "          output_path=model_artifacts_location,\n",
    "          checkpoint_s3_uri=model_artifacts_location, \n",
    "          train_instance_count=1,\n",
    "          train_instance_type=\"ml.p3.16xlarge\",\n",
    "          framework_version=\"1.6.0\",\n",
    "          py_version=\"py3\",\n",
    "          train_max_run=172800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "estimator.fit(\"s3://gluoncv-training/VOC-PlayingCards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  Model minimization with SageMaker Neo\n",
    " \n",
    " Neo enables machine learning models to train once and run anywhere in the cloud and at the edge.\n",
    " \n",
    " Neo consists of a <b>compiler</b> and a <b>runtime</b>\n",
    " \n",
    " The runtime is known as the DLR (Deep Learning Runtime).  It can be found here: https://github.com/neo-ai/neo-ai-dlr but is already installed on SageMaker hosting instances\n",
    " \n",
    " The compiler can be accessed through the CLI, from the SageMaker maker console or from the SageMaker SDK via a Notebook. \n",
    "We will use the latter method.\n",
    "\n",
    "Before we get to the model compilation however,  lets look at the components of a trained model in MXNET:\n",
    "\n",
    "model.params\n",
    "model-symbol.json\n",
    "\n",
    "The params file, as the name of it's extension implies saves the <i>parameters</i> of a trained model.  However it does not contain the model architecture. \n",
    "    \n",
    "The symbol.json file contains the hybridized model architecture.\n",
    "\n",
    "Gluon makes it possible to export a trained model without an architecture because model architecture cannot be saved for dynamic models since the model architecture changes during execution.\n",
    "\n",
    "If we refer to the Neo documentation we see that the Neo compiler requires files specific to the framework you are using:\n",
    "\n",
    "<b>Tensorflow</b>\n",
    "     \n",
    "     Neo supports saved models and frozen models.\n",
    "     For saved models, Neo expects one .pb or one .pbtxt file and a variables directory that contains variables.\n",
    "     For frozen models, Neo expect only one .pb or .pbtxt file.\n",
    "     \n",
    "<b>Keras</b>\n",
    "\n",
    "    Neo expects one .h5 file containing the model definition.\n",
    "\n",
    "<b>PyTorch</b>\n",
    "\n",
    "    Neo expects one .pth file containing the model definition.\n",
    "    \n",
    "<b> MXNET </b>\n",
    "\n",
    "    Neo expects one symbol file (.json) and one parameter file (.params).\n",
    "    \n",
    "In the next cell we will walk through, step-by-step, the process to export the <i>model architecture</i> from our freshly trained MXNet parameters file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets copy our .params file to local disk to work with it\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket, 'ssd_512_mobilenet1.0_custom_best.params', 'ssd_512_mobilenet1.0_custom_best.params')\n",
    "\n",
    "#### NOTE ####\n",
    "\n",
    "# 'ssd_512_mobilenet1.0_custom_best.params' comes from our train_ssd-playing-cards.py script.\n",
    "\n",
    "##############\n",
    "\n",
    "# Because we obtained this model's network from the GluonCV model zoo loading this .params file back into the MXNet framework \n",
    "# is a two step process:\n",
    "from gluoncv.model_zoo import get_model\n",
    "import mxnet as mx\n",
    "\n",
    "# First we get an instance of the network from the model zoo with the model_zoo get_model function\n",
    "my_model = get_model('ssd_512_mobilenet1.0_custom', pretrained=False, classes=my_classes, ctx=mx.gpu(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the parameters passed to the get_model function in the above cell\n",
    "\n",
    "<b> pretrained=False </b>\n",
    "\n",
    "    Since we specified the _custom network setting this to true would create unintended consequences.\n",
    "    if we had chosen ..._voc or ..._coco as the network and set this to true we would get a pretrained model\n",
    "\n",
    "<b> classes=my_classes </b>\n",
    "\n",
    "    This is a custom model trained on our 52 classes. So here we provide our classes to the network\n",
    "    \n",
    "<b> ctx=mx.gpu(0) </b>\n",
    "\n",
    "    ctx represents the ConTeXt that the model runs in.  This can can cpu or gpu. This context must match the context we\n",
    "    used during training\n",
    "\n",
    "In the next cell we will pass our <b>trained parameter file</b> into the my_model object to apply our parameters to the model zoo network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_parameters('ssd_512_mobilenet1.0_custom_best.params', ctx=mx.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the my_model object is now suitable for running inference within the MXNet framework, it is not yet ready for compilation in Sagemaker Neo.  Let's export the model architecture to a ...-symbol.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to symbolic format\n",
    "my_model.hybridize()\n",
    "\n",
    "# intialize the weights by passing a tensor (of zeros) of the correct shape\n",
    "my_model(mx.nd.ones((1, 3, 512, 512)).as_in_context(mx.gpu(0)))\n",
    "\n",
    "# Export the model architecture\n",
    "my_model.export('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final line of the above cell will create the two files we need for Neo\n",
    "\n",
    "the string value we passed to the export function is just the prefix.\n",
    "\n",
    "The files will be named:\n",
    "\n",
    "    my_model-0000.params\n",
    "    my_model-symbol.json\n",
    "    \n",
    "We can now compress these two files into the familiar model.tar.gz file (familiar if you have some experience with SageMaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "params = 'my_model-0000.params'\n",
    "symbols = 'my_model-symbol.json'\n",
    "tfile = \"model.tar.gz\"\n",
    "\n",
    "\n",
    "tar = tarfile.open(tfile, \"w:gz\")\n",
    "tar.add(params)\n",
    "tar.add(symbols)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to send our model to Neo to be recompiled.  \n",
    "\n",
    "Lets take a look at the data Neo requires to compile a model.  We will use a screenshot from the SageMaker console\n",
    "\n",
    "![alt text](images/NEO-compile.png \"Neo\")\n",
    "\n",
    "\n",
    "Fairly straightforward, but for the sake of completeness we will go through each item\n",
    "\n",
    "<b>job name </b>\n",
    "\n",
    "    A unique name to give the compilation job.  This will be visible from the SageMaker training jobs console. \n",
    "\n",
    "    \n",
    "<b> IAM Role </b>\n",
    "    \n",
    "    A role with sufficient permission to access SageMaker Neo.  \n",
    "  \n",
    "\n",
    "<b> Location of Model artifacts </b>\n",
    "\n",
    "    We outlined the Neo compiler required files earlier.  Here we will enter the location of the model.tar.gz file\n",
    "    we just created.  \n",
    "    \n",
    "<b> Data input Configuration </b>\n",
    "\n",
    "    This is the shape of an observation. It must be in NCHW format and wrapped.\n",
    "    N = Number of observations\n",
    "    C = Number of Channels\n",
    "    H = Height of the observation\n",
    "    W = Width of the observation\n",
    "    \n",
    "    Thus, our input configuration will be [1, 3, 512, 512]\n",
    "    and then wrapped it will become {\"data\": [1, 3, 512, 512]}\n",
    "    \n",
    "<b> Machine learning Framework </b>\n",
    "\n",
    "    The framework our model was trained in.  In this example, it is MXNET\n",
    "    \n",
    "<b> S3 Output Location </b>\n",
    "\n",
    "    The location where the compiled model will be placed upon completion\n",
    "    \n",
    "<b> Target Device </b>\n",
    "\n",
    "    You must tell NEO the type of device you will be deploying the model to.  For this example, we will use Jetson Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Since we only preserved the params object from our SageMaker training session, our estimator object does not have\n",
    "# knowledge of our newly exported model.\n",
    "model_key = 'model.tar.gz'\n",
    "\n",
    "# Lets move our model to persistant storage (s3) and then point our estimator to it.\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.upload_file(model_key, bucket, model_key)\n",
    "\n",
    "model_path = 's3://{}/{}'.format(bucket, model_key)\n",
    "compilation_job_name = 'jetson-mxnet-16-gluoncv-070'\n",
    "sm_client = boto3.client('sagemaker')\n",
    "data_shape = '{\"data\":[1,3,512,512]}'\n",
    "target_device = 'jetson_xavier'\n",
    "framework = 'MXNET'\n",
    "framework_version = '1.6.0'\n",
    "compiled_model_path = 's3://{}/neo-output'.format(bucket)\n",
    "\n",
    "response = sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': model_path,\n",
    "        'DataInputConfig': data_shape,\n",
    "        'Framework': framework\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': compiled_model_path,\n",
    "        'TargetDevice': target_device\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 300\n",
    "    }\n",
    ")\n",
    "print(response)\n",
    "\n",
    "# Poll every 30 sec\n",
    "while True:\n",
    "    response = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)\n",
    "    if response['CompilationJobStatus'] == 'COMPLETED':\n",
    "        break\n",
    "    elif response['CompilationJobStatus'] == 'FAILED':\n",
    "        raise RuntimeError('Compilation failed')\n",
    "    print('Compiling ...')\n",
    "    time.sleep(30)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
